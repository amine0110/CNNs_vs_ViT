[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://youtu.be/2i3g52ZGsZI) [![Website](https://img.shields.io/badge/WEBSITE-FFC800.svg?style=for-the-badge&logo=<badge>&logoColor=white)](https://pycad.co/cnns-or-vit-for-medical-imaging/) ![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)

# CNNs vs ViT - Medical Imaging Classification: Bone Fracture and Bone Age Detection

This repository contains the code and notebooks used in the experiment to classify medical images for bone fracture detection and bone age assessment using Convolutional Neural Networks (CNNs) and Vision Transformer (ViT).

![cover](https://github.com/amine0110/CNNs_vs_ViT/assets/37108394/f8ecfa40-4aef-43b1-8994-186843d945d4)

## Table of Contents

- [Introduction](#introduction)
- [Datasets](#datasets)
- [Methods](#methods)
- [Results](#results)
- [Getting Started](#getting-started)
- [Contact](#contact)
- [Subscribe to PYCAD Newsletter](#subscribe-to-pycad-newsletter)

## Introduction

This experiment aims to explore the effectiveness of CNNs and ViT models in classifying medical images for two different tasks:

1. Bone Fracture Detection
2. Bone Age Assessment

## Datasets

We used the following datasets:

1. [Bone Fracture Detection Dataset](https://www.kaggle.com/datasets/vuppalaadithyasairam/bone-fracture-detection-using-xrays)
2. [Bone Age Detection Dataset](https://www.kaggle.com/datasets/kmader/rsna-bone-age)

## Methods

We trained several CNNs models such as Xception, InceptionV3, VGG16, and ResNet for the classification tasks. We also trained a Vision Transformer model for the same.

- CNNs: Code taken from [this repository](https://github.com/amine0110/medical-imaging-classification)
- ViT: Code adapted from [this repository](https://github.com/marcellusruben/medium-resources/blob/main/ViT/Vision_Transformer.ipynb)

## Results

While CNN models didn't produce optimal results, the Vision Transformer model achieved over 94% accuracy in only 10 epochs.

## Getting Started

Both notebooks are compatible with Google Colab and include a button to run the code there.

1. Clone this repository.
2. Open the notebook you are interested in.
3. Click the "Run in Google Colab" button to execute the notebook.

## Contact

For questions related to this experiment or medical imaging in general, feel free to book a premium session through [PYCAD Services](https://pycad.co/services).

## Subscribe to PYCAD Newsletter

Stay updated with the latest advancements in medical imaging by subscribing to our [Newsletter](https://pycad.co/join-us).
